Different approach:
Copy entire blocks.
Upon detecting block, copy to new memory block pool.

Hmm, could directly patch like in valgrind.
My approach makes indirect jumps faster -- don't have to do translation.

What to do when new block b is discovered:
1. Allocate b' in code pool.
2. Set breakpoint at end. Upon reaching breakpoint, code will then patch the next block.

Whenever a breakpoint is reached, find next


COMPONENTS:
[ ] Block class.
[ ] Set of blocks holding original code.


;;;;;;

Preserve origianl code.
Completely rewrite.
Translate orig block addresses to rewritten block addresses.
Jump directly there.

DIRECT, UNCONDITONAL JUMPS: Easy. Just jump to next rewritten block. This includes CALLs!
DIRECT, CONDITONAL JUMPS: Use conditional jump to next rewritten block. Then either (1) BKPT or (2) unconditional jump to fallthrough block.
INDIRECT JUMPS: bkpt for now to translate into rewritten block address. This includes CALLs and RETs!


HMM..
Don't overuse classes. Makes problem even harder.

[ ] Map from orig addresses to blocks.

---------------

Simplify block instructions by putting it into one instruction stream.
Observation: Only one breakpoint will be active at once. For direct branches, the direct branch breakpoint will be hit exactly once.

Improvement:
Don't patch positive conditional jumps unless branch is actually taken, direct or indirect.
Currently, the tool patches direct jumps eagerly, but this isn't always correct.

----------

Block Terminator Instruction Stages:

Block creation:
- Fix up call.
- Branch to BKPT.
- Fallthrough is BKPT.

Hmm... Could have terminator be other class with virtual methods... try this.

Hmmm
How the hell to create relocatable instructions?
Just save data, xedd,

----

Problem: RIP-relative memory jumps are difficult.
Solution: just jump to original address then single-step instruction.
	  If call, pop off stack push.
---------

Hmm, instead of transforming ANY indirect branch instructions, can just jump directly to the indirect branch branch instruction for simplicity.

Wait, can just jump to calls, since they (1) never fallthru and (2) it will push the right value onto the stack.

---------

Bug: RIP-relative data accesses don't work.
Possible fix: Create shadow copies. But this is bad -- what about RIP-relative arrays?
Possible fix: push rax \ mov rax, [rip+...] \ xchg [rsp], rax

--------

Performance issue: RETs are common but slow.
Idea: use RSB-style optimization.
On each CALL, push return value onto RSB.
On each RET, pop return value from RSB. If actual return value equals predicted return value, go directly.

Without bounds checking:

CALL_RELBRd:
push <orig_ra>
push rax
mov rax, rsp
mov rsp, [rel rsb_ptr]
push [rel new_ra]
push [rel orig_ra]
mov [rel rsb_ptr], rsp
pop rax
{JMP}

RET:
xchg rax, [rsp]
push rcx
mov rcx, [rel rsb_ptr]
add qword [rel rsb_ptr], 16
cmp rax, [rcx]
jne mismatch
mov rax, [rcx + 8]
pop rcx
xchg rax, [rsp]
OAret

mismatch:
   bkpt

With bounds checking:

CALL_RELBRd:
 push <orig_ra>
 push rax
 mov rax, rsp
 mov rsp, [rel rsb_ptr]
 cmp rsp, [rel rsb_end]
 je after_entry         ; full, so bypass store of new entry
 push [rel new_ra]
 push [rel orig_ra]
 mov [rel rsb_ptr], rsp
after_entry:
 mov rsp, rax
 pop rax
 {JMP}

RET:
 xchg rax, [rsp]
 push rcx
 mov rcx, [rel rsb_ptr]
 cmp rcx, [rel rsb_begin]
 je mismatch                  ; RSB empty
 add qword [rel rsb_ptr], 16
 cmp rax, [rcx]
 jne mismatch                 ; mismatch
 mov rax, [rcx + 8]
 pop rcx
 xchg rax, [rsp]
 ret

mismatch:
 bkpt

------

Optimizations:
Indirect jmps/calls also need cache.

-----

1   0.500
2   0.461
3   0.446
4   0.440
8   0.433

---

1.5 0.422
1.6 0.425
1.7 0.425
1.8 0.430

-------

Block::Create:

For each instruction, call the handler on the original instruction.
The handler is provided with a writer that accepts blobs.
The blob is relocated using a special rule that translates RIP-relative instructions.
Furthermore, if the blob is a branch instruction, the loop terminates and the terminator is created.

The instruction writer for now should just directly write to memory. In the future, it should use a buffer.

--------

Able to detect when stack changes occur. How about when system calls occur?

-----

Need to save all memory somehow.

Use mprotect to make memory read-only.
On write, save page.


----

Critical event?
SYSCALL.

When a syscall occurs:
1. Rewind to previous state.
2. Handle system call if mmap/etc.
3.


-----

Need mask of tainted memory.
Then need to interpret system call.

Taint state variable represents mask of current tainted bits.

Before each subround, invert tainted bits.
Two subrounds per round.
xor states together to get tainted state.
Interpret system call to update tainted memory.

------

Have master memory hierarchy representation.


-----

Just do freaking page table. Freaking duh.
Snapshot will have an unordered map of page addresses to page data.

------

Ok, so now need to check to make sure that inputs to syscall aren't uninitialized.

----

Need to also check that REGISTER inputs to syscall aren't uninitialized. This is more challenging using macros -- need to revise macro system.
Include new argument count element. 'sysx' should then add the correct expansion afterwards.

---

Treat rdtsc as an 'event' instruction

----

Hard to debug why false positives are occurring. Need side-by-side execution functionality to debug further.
Also need to implement stack randomization.

---

Randomize when SP decreases doesn't work for accesses below SP.
A solution: randomize whenever SP increases, not decreases. (?)

I investigated how valgrind deals with shadow stack memory.
Apparently it looks for a CALL or RET.
If a CALL or RET is encountered, it ALWAYS marks the shadow stack memory as uninitialized.

Also, valgrind ignores changes in the stack pointer inside functions. I.e. if SP is incremented by 128, a signal could overwrite the values, but valgrind doesn't complain.

So basically the entire stack from SP

On CALL/RET: mark mem below SP as tainted.
All references below SP-128 invalid. (Can't do this yet.)

---

